{
  "_args": [
    [
      {
        "raw": "crawler@^1.0.5",
        "scope": null,
        "escapedName": "crawler",
        "name": "crawler",
        "rawSpec": "^1.0.5",
        "spec": ">=1.0.5 <2.0.0",
        "type": "range"
      },
      "C:\\Users\\Administrator\\Desktop\\jdshopping"
    ]
  ],
  "_from": "crawler@>=1.0.5 <2.0.0",
  "_id": "crawler@1.0.5",
  "_inCache": true,
  "_location": "/crawler",
  "_nodeVersion": "6.9.1",
  "_npmOperationalInternal": {
    "host": "packages-12-west.internal.npmjs.com",
    "tmp": "tmp/crawler-1.0.5.tgz_1494417107443_0.3716903864406049"
  },
  "_npmUser": {
    "name": "mike442144",
    "email": "mike442144@hotmail.com"
  },
  "_npmVersion": "3.10.8",
  "_phantomChildren": {},
  "_requested": {
    "raw": "crawler@^1.0.5",
    "scope": null,
    "escapedName": "crawler",
    "name": "crawler",
    "rawSpec": "^1.0.5",
    "spec": ">=1.0.5 <2.0.0",
    "type": "range"
  },
  "_requiredBy": [
    "/"
  ],
  "_resolved": "https://registry.npmjs.org/crawler/-/crawler-1.0.5.tgz",
  "_shasum": "3a95f1bff46763e57ee6161d012b967f8e771684",
  "_shrinkwrap": null,
  "_spec": "crawler@^1.0.5",
  "_where": "C:\\Users\\Administrator\\Desktop\\jdshopping",
  "bugs": {
    "url": "https://github.com/bda-research/node-crawler/issues"
  },
  "dependencies": {
    "bottleneckp": "~1.1.2",
    "charset-parser": "^0.2.0",
    "cheerio": "^0.22.0",
    "iconv-lite": "^0.4.8",
    "lodash": "^4.17.4",
    "request": "~2.79.0",
    "seenreq": "^0.1.7",
    "type-is": "^1.6.14"
  },
  "description": "Crawler is a web spider written with Nodejs. It gives you the full power of jQuery on the server to parse a big number of pages as they are downloaded, asynchronously",
  "devDependencies": {
    "chai": "^2.3.0",
    "jsdom": "^9.6.0",
    "mocha": "^2.2.5",
    "mocha-testdata": "^1.1.0",
    "sinon": "^1.14.1",
    "whacko": "^0.19.1"
  },
  "directories": {
    "test": "tests"
  },
  "dist": {
    "shasum": "3a95f1bff46763e57ee6161d012b967f8e771684",
    "tarball": "https://registry.npmjs.org/crawler/-/crawler-1.0.5.tgz"
  },
  "engine-strict": {
    "node": ">=4.0.0"
  },
  "gitHead": "0f3a27d4e89d93fcf9c27a417adbba6fab9dde2e",
  "homepage": "https://github.com/bda-research/node-crawler",
  "keywords": [
    "dom",
    "javascript",
    "crawling",
    "spider",
    "scraper",
    "scraping",
    "jquery",
    "crawler",
    "nodejs"
  ],
  "licenses": [
    {
      "type": "MIT",
      "url": "http://github.com/bda-research/node-crawler/blob/master/LICENSE.txt"
    }
  ],
  "main": "./lib/crawler.js",
  "maintainers": [
    {
      "name": "darrenqc",
      "email": "darrenqc823@gmail.com"
    },
    {
      "name": "mike442144",
      "email": "mike442144@hotmail.com"
    },
    {
      "name": "paulvalla",
      "email": "bonjour@pol.ninja"
    },
    {
      "name": "sylvinus",
      "email": "sylvain@sylvainzimmer.com"
    }
  ],
  "name": "crawler",
  "optionalDependencies": {},
  "readme": "ERROR: No README data found!",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/bda-research/node-crawler.git"
  },
  "scripts": {
    "test": "./node_modules/mocha/bin/mocha --reporter spec --bail --timeout 10000 tests/*.js"
  },
  "version": "1.0.5"
}
